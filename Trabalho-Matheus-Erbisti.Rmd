---
title: ""
output:
  pdf_document:
    includes:
      in_header: D:\MATHEUS\Estatística\5º semestre\Estatística Computacional\Trabalho\Trabalho Matheus Erbisti\head.tex
bibliography: bib.bib
link-citations: true
nocite: | 
  @ref1, @ref2, @ref3, @ref4, @ref5
---

\centering
\raggedright
\begin{center}
```{r pressure, echo=FALSE,out.width = '50%',fig.align='center'}
knitr::include_graphics("unb.jpg")
```
 \Large Universidade de Brasília\\
 IE - Departamento de Estatística\\
 Trabalho de Estatística Computacional - 1/2020
\end{center} 
 \vskip 12em
\begin{center}
 \Large \textbf{Simulação do Viés e Erro Quadrático Médio dos estimadores de máxima verossimilhança dos parâmetros da distribuição Power Lindley}
 \par
 \vskip 7em
\end{center}
\setlength{\baselineskip}{.5cm}
\small \textbf{}
\par
\vskip 5em

\begin{flushright}
\small Matheus Erbisti Pontes - 180024990\\
\vskip 2em
\small Prof. Helton Saulo
\end{flushright}

\vskip 6em
\begin{center}
\setlength{\baselineskip}{.5cm}
Brasília\\
\vskip 1em
Dezembro de 2020
\end{center}
\newpage
\renewcommand{\contentsname}{Sumário}
\tableofcontents
\newpage

# Introdução

\justify

\fontsize{13pt}{20pt}\selectfont

Este estudo tem como objetivo avaliar dois diferentes algoritmos de geração de números aleatórios da distribuição Power Lindley, através dos estimadores de máxima verossimilhança e por simulações de Monte Carlo.

Os critérios a serem avaliados serão o viés e o erro quadrático médio, para diferentes parâmetros e tamanhos de amostra gerados por dois de três algoritmos propostos em "Power Lindley distribution and associated inference" (ver referências).

Dessa forma, busca-se reproduzir valores próximos dos encontrados nas tabelas 1 e 2 do referido artigo, porém para os algoritmos 1 e 3 propostos. Todo esse documento, inclusive os códigos, foram redigidos no software R 4.0.3, através do RStudio 1.3.1093.

\newpage

# As distribuições Lindley e Power Lindley

## A distribuição de Lindley

A distribuição de Lindley foi proposta em 1958, carregando o nome de seu autor. Ela possui a seguinte função de densidade de probabilidade:

$$f_{1}(t) = \frac{\beta^2}{\beta + 1} (1 + t) e^{-\beta t}, t > 0, \beta > 0$$

Ou, alternativamente:

$$f_{1}(t) = p \xi_{1} + (1 - p) \xi_{2}, \text{ onde } p = \frac{\beta}{\beta + 1}, \text{ } \xi_{1}(t) = \beta e^{-\beta t}, \text{ } \xi_{2}(t) = \beta^2 t e^{-\beta t}$$

Dessa forma, é possível perceber que a distribuição de Lindley nada mais é do que uma "mistura" entre as distribuições Exponencial($\beta$) e Gama(2, $\beta$), e com a proporção de "mistura" $p$.

## A distribuição Power Lindley

Contudo, a distribuição de Lindley nem sempre se ajusta bem a modelos práticos e teóricos e, por isso, desenvolveu-se a distribuição Power Lindley, que é gerada através uma transformação de potência $X = T^{1/ \alpha}$ aplicada na distribuição de Lindley.

Sendo assim, temos a seguinte função de densidade de probabilidade para a distribuição Power Lindley:

$$f(x) = \frac{\alpha \beta^2}{\beta + 1} (1 + x^{\alpha}) x^{\alpha - 1} e^{-\beta x^{\alpha}}, \text{ } x, \text{ } \alpha \text{ e } \beta > 0$$

Similarmente a distribuição de Lindley, a Power Lindley também possui uma versão alternativa:

$$f(x) = p g_{1} + (1 - p) g_{2}, \text{ onde } p = \frac{\beta}{\beta + 1}, \text{ } g_{1}(x) = \alpha \beta x^{\alpha - 1} e^{-\beta x^{\alpha}}, \text{ } g_{2}(x) = \alpha \beta^2 x^{2 \alpha - 1} e^{-\beta x^{\alpha}}$$

Agora, vemos que a Power Lindley também pode ser decomposta como a "mistura" entre outras duas distribuições, no caso a Weibull($\alpha, \beta$) e Gama generalizada (2, $\alpha, \beta$) e a mesma proporção de "mistura" $p$.

Abaixo, temos o gráfico da distribuição PL com os determinados valores de parâmetros que utilizaremos durante todo esse documento.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
n <- c(25,50, 75, 100, 200); N <- 10000
alphas <- c(0.2, 1.5, 0.9); betas <- c(4, 1, 0.35)
library(ggplot2)

#Função densidade Power Lindley
dplindley <- function(x, alpha, beta){
  
  p <- ((alpha * beta^2) / (beta + 1)) * (1 + x^alpha) * x^(alpha - 1) *
         exp(-beta * (x^alpha))
  
  return(p)
}

#Gráfico Densidade Power Lindley

legenda <- c(bquote(alpha == .(alphas[1]) ~ ", " ~ beta == .(betas[1])),
            bquote(alpha == .(alphas[2]) ~ ", " ~ beta == .(betas[2])), 
            bquote(alpha == .(alphas[3]) ~ ", " ~ beta == .(betas[3])))

x1 <- seq(0, 4, 0.0001)
df <- data.frame(x = x1, y = c(y1 = dplindley(x1, alphas[1], betas[1]),
                               y2 = dplindley(x1, alphas[2], betas[2]),
                               y3 = dplindley(x1, alphas[3], betas[3])), 
                               group = factor(rep(1:3, each = 40001)))
                 
ggplot(df, aes(x = x, y = y, group = group, colour = group)) +
  geom_line(size = 1) +
  scale_colour_manual(name = "", values = c("#A11D21", "#003366", "green"), labels = legenda) +
  labs(x = "x", y = "Função Densidade de Probabilidade") +
  theme_bw() +
  theme(axis.title.y = element_text(colour = "black", size = 12),
        axis.title.x = element_text(colour = "black", size = 12),
        axis.text = element_text(colour = "black", size = 9.5),
        panel.border = element_blank(),
        axis.line = element_line(colour = "black"),
        legend.background = element_blank(),
        legend.text = element_text(size = 15),
        legend.position = c(0.55,0.85)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(0, 2))
```


\newpage

# Estimação e estimadores

Considere uma população com parâmetro $\theta$ (por exemplo, a média ou variância dessa população) e uma respectiva amostra $X_{1}, X_{2}, ..., X_{n}$. Sendo assim, conhecer $\theta$ traz importantes informações sobre a população, e queremos descobrir mais sobre $\theta$ através da amostra coletada.

O processo de descobrir informações de um parâmetro através de uma amostra é o que chamamos de estimação de um parâmetro, isso é, queremos encontrar informações $\theta$ na amostra e poder generalizar isso para a população.

Por definição, um estimador T é qualquer função das observações da amostra, isso é, $T = g(X_{1}, X_{2}, ..., X_{n})$. Dessa maneira, é de fundamental importância desenvolver um estimador $T = g(X_{1}, X_{2}, ..., X_{n})$ que seja o mais próximo possível de $\theta$, ou seja, ter um estimador que retorne informações confiáveis de $\theta$.

Existem diversas maneiras de se obter estimadores de $\theta$, mas de maneira geral queremos sempre estimadores que tenham as seguintes propriedades:

* **Consistência:** Significa que quanto maior a amostra, mais próxima a estimativa calculada estará do valor real do parâmetro. Ou seja, $P \{ |T_{n} > \theta| \} \rightarrow 0, n \rightarrow \infty$

* **Não-viesado:** O valor esperado de um estimador é o próprio parâmetro. Em outras palavras, um estimador não-viesado em média, deve sempre apontar para o valor verdadeiro do parâmetro. $\rightarrow E(T) = \theta$

* **Eficiência:** O estimador tem a menor variância possível. Isso é, calculando uma estimativa com diferentes dados e o mesmo estimador, a grande maioria dos valores dessa estimativa será próxima de um valor específico.

## Critérios de avaliação de estimadores

Existem diversos critérios para avaliar as propriedades de estimadores, contudo aqui iremos focar em apenas dois dos mais populares e que serão aplicados na simulação deste trabalho.

### Viés

Por definição, o viés de um estimador é a diferença do valor esperado do estimador e o próprio parâmetro, i.e., $\text{Viés}(\hat{\theta}) = E[\hat{\theta}] - \theta$. Note que se um estimador é não-viesado, seu viés é equivalente a zero, e obtemos a identidade $E[\hat{\theta}] = \theta$. Nesse estudo, verificaremos o viés do estimador utilizado de maneira empírica, através de inúmeras simulações com diferentes parâmetros e tamanhos de amostras.

### Erro Quadrático Médio (EQM)

A definição do Erro Quadrático Médio afirma que ele é a diferença quadrática média entre o estimador e seu respectivo parâmetro, ou seja, $\text{EQM}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2]$. Uma das vantagens dessa medida de avaliação de estimadores é que ela pode ser decomposta como a soma da variância e o quadrado do viés desse mesmo estimador, da seguinte forma:

$$\text{EQM}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = \text{VAR}(\hat{\theta} - \theta) - E^2[\hat{\theta} - \theta] = \big[ (\text{VAR}(\hat{\theta}) - \text{VAR}(\theta) \big] - \big[ E^2[\hat{\theta} ] - E^2[\theta] \big]$$
$$\text{EQM}(\hat{\theta}) = \text{VAR}(\hat{\theta}) - \text{Viés}^2(\hat{\theta})$$

## Estimadores de Máxima Verossimilhança

A máxima verossimilhança é uma das técnicas de estimação mais populares. O princípio por trás desse método é que devemos escolher os valores dos parâmetros que maximizam a probabilidade de termos obtido tal amostra de dados. Com esse intuito, essa técnica consiste em descobrir os estimadores de máxima verossimilhança através da maximização da chamada "função de verossimilhança".

Em relação à propriedades desejáveis desses estimadores, temos:

* São consistentes
* Quando $n \rightarrow \infty$, os estimadores de máxima verossimilhança são não-viesados.
* Se $n \rightarrow \infty$, os EMV se tornam eficientes.

Sendo assim, vamos designar a partir de agora PL($\alpha, \beta$) como uma distribuição de Power Lindley com parâmetros $\alpha$ e $\beta$. Então, temos o seguinte processo para obter os estimadores de máxima verossimilhança da PL($\alpha, \beta$):

Seja $x_{1}, x_{2}, ..., x_{n}$ uma amostra de tamanho n de uma PL($\alpha, \beta$). Sendo assim, a função de verossimilhança será:

$$L(\alpha, \beta | x_{i}) = \prod_{i = 1}^{n} f(x_{i}) = \prod_{i = 1}^{n} \frac{\alpha \beta^2}{\beta + 1} (1 + x^{\alpha}) x^{\alpha - 1} e^{-\beta x^{\alpha}}$$

Agora, como a função logarítimica é estritamente crescente no intervalo de  $L(\alpha, \beta | x_{i})$, encontrar o máximo de $\ln (L(\alpha, \beta | x_{i}))$ equivale a encontrar o máximo de $L(\alpha, \beta | x_{i})$. Então, aplicaremos a função $\ln()$ para facilitar manipulações matemáticas, tornando a função de verossimilhança em log-verossimilhança. Portanto:

$$\ln (L(\alpha, \beta | x_{i})) = \sum_{i = 1}^{n} \ln \Bigg( \frac{\alpha \beta^2}{\beta + 1} (1 + x^{\alpha}) x^{\alpha - 1} e^{-\beta x^{\alpha}} \Bigg)$$
$$= n[\ln(\alpha) + 2 \ln(\beta) - ln(\beta + 1)] + \sum_{i = 1}^{n} \ln(1 + x_{i}^{\alpha}) + (\alpha - 1) \sum_{i = 1}^{n} \ln(x_{i}) - \beta \sum_{i = 1}^{n} x_{i}^{\alpha}$$

Assim sendo, para encontrar os estimadores de máxima verossimilhança $\hat{\alpha}$ e $\hat{\beta}$ temos que derivar a função de log-verossimilhança em relação a $\alpha$ e $\beta$ respectivamente, igualar a zero e isolar $\alpha$ e $\beta$ respectivamente. Logo:

$$\frac{\partial}{\partial \alpha} \ln (L(\alpha, \beta | x_{i})) = \frac{n}{\alpha} + \sum_{i = 1}^{n} \frac{x_{i}^{\alpha} \ln(x_{i})}{1 + x_{i}^{\alpha}} + \sum_{i = 1}^{n} \ln(x_{i}) - \beta \sum_{i = 1}^{n} x_{i}^{\alpha} \ln(x_{i}) = 0$$

$$\frac{\partial}{\partial \beta} \ln (L(\alpha, \beta | x_{i})) = \frac{n (\beta + 2)}{\beta (\beta + 1)} - \sum_{i = 1}^{n} x_{i}^{\alpha} = 0$$

Podemos transformar a última equação no formato de uma equação do 2º grau, da seguinte maneira:

$$\frac{n (\beta + 2)}{\beta (\beta + 1)} - \sum_{i = 1}^{n} x_{i}^{\alpha} = 0$$
$$n (\beta + 2) -  (\beta^2 + \beta) \sum_{i = 1}^{n} x_{i}^{\alpha} = 0$$
$$\beta n + 2n -  \beta^2 \sum_{i = 1}^{n} x_{i}^{\alpha} - \beta \sum_{i = 1}^{n} x_{i}^{\alpha} = 0$$
$$\Bigg( \sum_{i = 1}^{n} x_{i}^{\alpha} \Bigg) \beta^2 + \beta \Bigg( \sum_{i = 1}^{n} x_{i}^{\alpha} - n \Bigg) + 2n = 0$$

Com isso, podemos encontrar através da fórmula de Bhaskara a seguinte solução:

$$\hat{\beta}(\hat{\alpha}) = \frac{- \Bigg( \sum_{i = 1}^{n} x_{i}^{\hat{\alpha}} - n \Bigg) + \sqrt{\Bigg( \sum_{i = 1}^{n} x_{i}^{\hat{\alpha}} - n \Bigg)^2 + 8n \sum_{i = 1}^{n} x_{i}^{\hat{\alpha}}}}{2 \sum_{i = 1}^{n} x_{i}^{\hat{\alpha}}}$$

Onde $\hat{\alpha}$ é a solução da seguinte equação:

$$G(\alpha) = \frac{n}{\alpha} + \sum_{i = 1}^{n} \frac{x_{i}^{\alpha} \ln(x_{i})}{1 + x_{i}^{\alpha}} + \sum_{i = 1}^{n} \ln(x_{i}) - \hat{\beta}(\alpha) \sum_{i = 1}^{n} x_{i}^{\alpha} \ln(x_{i}) = 0$$

\newpage


# Geração de Números Pseudo-Aleatórios

Um gerador de números pseudo-aleatórios é um algoritmo que retorna uma sequência de números que podem ser utilizados como se fossem uma sucessão iid de verdadeiros números aleatórios. Nesse trabalho, utilizaremos duas técnicas de geração de NPAs.

## Transformação e mistura de variáveis aleatórias

Existem diversas transformações e misturas para diversas v.a.'s, porém, estamos interessados na distribuição PL($\alpha$, $\beta$). Dessa maneira, já foi demonstrado que a Power Lindley é uma transformação de potência da distribuição Lindley, que por sua vez é uma mistura de outras duas distribuições. Sendo assim, podemos montar o seguinte algoritmo para gerar números aleatórios de uma 
PL($\alpha$, $\beta$):

### Algoritmo 1

1. Gerar n números da distribuição $U \sim Uniforme(0,1)$.
2. Como a Lindley($\beta$) é uma mistura das distribuições $E \sim Exponencial(\beta)$ e $G \sim Gama(2, \beta)$, gerar n números dessas duas distribuições.
3. Para todos os n números gerados da Uniforme, testar se $U_{i} \leq \frac{\beta}{\beta + 1}$, onde i = 1, 2, ..., n.
4. Se a desigualdade for verdadeira, então aplicar uma transformação $X_{i} = E^{1/ \alpha}$ para o i-ésimo número verificado dentre todos os n, onde $X \sim PL(\alpha, \beta)$.
5. Caso a desigualdade for falsa, aplicar então a transformação $X_{i} = G^{1/ \alpha}$ para o i-ésimo número verificado, i = 1, 2, ..., n.

```{r echo=FALSE}
#Algoritmo 1

Algoritmo1 <- function(n, alpha, beta){

  x <- numeric(n)
u <- runif(n); e <- rexp(n, beta); g <- rgamma(n, 2, rate = beta)

for (i in 1:n) {
  if(u[i] <= beta / (beta + 1)){
    
    x[i] <- e[i]^(1 / alpha)
    
  } else {
    
    x[i] <- g[i]^(1 / alpha)
    
  }}

return(x)
}
```


## Método da transformada inversa

Embora essa técnica de geração de NPAs também seja uma transformação, ela merece maior destaque nesse trabalho, pois o último algoritmo pertence a essa classificação. O teorema da transformação inversa diz que:

*Seja X uma variável aleatória contínua com função de distribuição acumulada (FDA) F(x) com inversa $F^{-1}$(u), para todo u $\in$ (0,1), e seja U $\sim$ U[0,1].*

*Defina-se X := $F^{-1}$(U), então X tem FDA F.*

No caso desse algoritmo para a $PL(\alpha, \beta)$, isso se traduz da seguinte forma:

### Algoritmo 3

1. Gerar n números da distribuição $U \sim Uniforme(0,1)$.
2. Considerar

$$X = \Bigg[ -1 - \frac{1}{\beta} - \frac{1}{\beta} W_{-1} \Bigg( - \frac{\beta + 1}{e^{\beta + 1}} (1 - U) \Bigg) \Bigg]^{1/ \alpha}$$

para todos os n números gerados, onde $W_{-1}$ é a parte negativa da função W de Lambert.

```{r echo=FALSE}
#Algoritmo 3

Algoritmo3 <- function(n, alpha, beta){
  x <- numeric(n)
  u <- runif(n)
  
  for (i in 1:n) {
    
    x[i] <- (-1 -(1/beta) - (1/beta)*lamW::lambertWm1(((beta + 1) / exp(beta + 1)) * (1 - u[i]) * (-1)))^(1 / alpha)
    
  }
  
  return(x)
  }
```


\newpage

# Resultados e Conclusões

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#Função log-verossimilhança Power Lindley
loglik <- function(vetor, x){

   LL <- sum(log(dplindley(x = x, alpha = vetor[1], beta = vetor[2])))
   
return(-LL)
}


#Loop Monte-Carlo Algoritmo 1

df1 <- data.frame()

for (k in 1:3) {

for (j in 1:5){
  
  soma_parametros <- c(0, 0)
  soma_var <- c(0,0)
  
  for(i in 1:N){
    
    x <- Algoritmo1(n[j], alphas[k], betas[k])
    fit <- suppressWarnings(optim(c(alphas[k], betas[k]), loglik, x = x,
                                  method = "Nelder-Mead")$par)
    
    
    soma_parametros <- soma_parametros + fit
    soma_var <- soma_var + (fit - c(alphas[k], betas[k]))^2
  }
  
  vies_alpha <- round(soma_parametros[1]/N - alphas[k], 4)
  eqm_alpha <-  round(soma_var[1]/N + vies_alpha^2, 4)
  vies_beta <-  round(soma_parametros[2]/N - betas[k], 4)
  eqm_beta <-  round(soma_var[2]/N + vies_beta^2, 4)
  
  df1 <- rbind(df1, c(alphas[k], betas[k], n[j], vies_alpha, eqm_alpha, vies_beta,
              eqm_beta))
}
}

names(df1)[1] <- "Alpha"; names(df1)[2] <- "Beta"; names(df1)[3] <- "n"
names(df1)[4] <- "Vies Alpha"; names(df1)[5] <- "EQM Alpha"
names(df1)[6] <- "Vies Beta"; names(df1)[7] <- "EQM Beta"

knitr::kable(df1, caption = "Tabela do Algoritmo 1")


#Loop Monte-Carlo Algoritmo 3

df3 <- data.frame()

for (k in 1:3) {

for (j in 1:5){
  
  soma_parametros <- c(0, 0)
  soma_var <- c(0,0)
  
  for(i in 1:N){
    
    x <- Algoritmo3(n[j], alphas[k], betas[k])
    fit <- suppressWarnings(optim(c(alphas[k], betas[k]), loglik, x = x,
                                  method = "Nelder-Mead")$par)
    
    
    soma_parametros <- soma_parametros + fit
    soma_var <- soma_var + (fit - c(alphas[k], betas[k]))^2
  }
  
  vies_alpha <- round(soma_parametros[1]/N - alphas[k], 4)
  eqm_alpha <-  round(soma_var[1]/N + vies_alpha^2, 4)
  vies_beta <-  round(soma_parametros[2]/N - betas[k], 4)
  eqm_beta <-  round(soma_var[2]/N + vies_beta^2, 4)
  
  df3 <- rbind(df3, c(alphas[k], betas[k], n[j], vies_alpha, eqm_alpha, vies_beta,
              eqm_beta))
}
}

names(df3)[1] <- "Alpha"; names(df3)[2] <- "Beta"; names(df3)[3] <- "n"
names(df3)[4] <- "Vies Alpha"; names(df3)[5] <- "EQM Alpha"
names(df3)[6] <- "Vies Beta"; names(df3)[7] <- "EQM Beta"

knitr::kable(df3, caption = "Tabela do Algoritmo 3")
```

Observando as tabelas 1 e 2 acima, verificamos que:

* Ambos os algoritmos reproduziram valores próximos de Viés e EQM com os dados tamanhos de amostra e parâmetros.

* Os resultados obtidos são próximos dos vistos no artigo-base deste seminário.

* A estimação de beta é bem mais sensível a um tamanho pequeno de amostra em relação a estimação de alpha, tendo viés e EQM consideravelmente superiores para um mesmo tamanho de amostra.

* Independente dos valores de alpha e beta, o erro quadrático médio e o viés dos dois parâmetros tendem a diminuir conforme aumenta-se o tamanho da amostra, evidenciando uma das propriedades dos estimadores de máxima verossimilhança.


\newpage

# Códigos utilizados

\fontsize{10pt}{15pt}\selectfont

```{r eval=F}
n <- c(25, 50, 75, 100, 200); N <- 10000
alphas <- c(0.2, 1.5, 0.9); betas <- c(4, 1, 0.35)
library(ggplot2)

#Função densidade Power Lindley
dplindley <- function(x, alpha, beta){
  
  p <- ((alpha * beta^2) / (beta + 1)) * (1 + x^alpha) * x^(alpha - 1) *
         exp(-beta * (x^alpha))
  
  return(p)
}

#Gráfico Densidade Power Lindley

my.lab <- c(bquote(alpha == .(alphas[1]) ~ ", " ~ beta == .(betas[1])),
            bquote(alpha == .(alphas[2]) ~ ", " ~ beta == .(betas[2])), 
            bquote(alpha == .(alphas[3]) ~ ", " ~ beta == .(betas[3])))

x1 <- seq(0, 4, 0.0001)
df <- data.frame(x = x1, y = c(y1 = dplindley(x1, alphas[1], betas[1]),
                               y2 = dplindley(x1, alphas[2], betas[2]),
                               y3 = dplindley(x1, alphas[3], betas[3])), 
                               group = factor(rep(1:3, each = 40001)))
                 
ggplot(df, aes(x = x, y = y, group = group, colour = group)) +
  geom_line(size = 1) +
  scale_colour_manual(name = "", values = c("#A11D21", "#003366", "green"),
                      labels = my.lab) +
  labs(x = "x", y = "Função Densidade de Probabilidade") +
  theme_bw() +
  theme(axis.title.y = element_text(colour = "black", size = 12),
        axis.title.x = element_text(colour = "black", size = 12),
        axis.text = element_text(colour = "black", size = 9.5),
        panel.border = element_blank(),
        axis.line = element_line(colour = "black"),
        legend.background = element_blank(),
        legend.text = element_text(size = 15),
        legend.position = c(0.55,0.85)) +
  scale_x_continuous(limits = c(0, 4)) +
  scale_y_continuous(limits = c(0, 2))


#Algoritmo 1

Algoritmo1 <- function(n, alpha, beta){

  x <- numeric(n)
u <- runif(n); e <- rexp(n, beta); g <- rgamma(n, 2, rate = beta)

for (i in 1:n) {
  if(u[i] <= beta / (beta + 1)){
    
    x[i] <- e[i]^(1 / alpha)
    
  } else {
    
    x[i] <- g[i]^(1 / alpha)
    
  }}

return(x)
}


#Algoritmo 3

Algoritmo3 <- function(n, alpha, beta){
  x <- numeric(n)
  u <- runif(n)
  
  for (i in 1:n) {
    
    x[i] <- (-1 -(1/beta) - (1/beta)*
            lamW::lambertWm1(((beta + 1) / exp(beta + 1))* (1 - u[i]) * (-1)))^(1 / alpha)
    
  }
  
  return(x)
}


#Função log-verossimilhança Power Lindley
loglik <- function(vetor, x){

   LL <- sum(log(dplindley(x = x, alpha = vetor[1], beta = vetor[2])))
   
return(-LL)
}


#Loop Monte-Carlo Algoritmo 1

df1 <- data.frame()

for (k in 1:3) {

for (j in 1:5){
  
  soma_parametros <- c(0, 0)
  soma_var <- c(0,0)
  
  for(i in 1:N){
    
    x <- Algoritmo1(n[j], alphas[k], betas[k])
    fit <- suppressWarnings(optim(c(alphas[k], betas[k]), loglik, x = x,
                                  method = "Nelder-Mead")$par)
    
    
    soma_parametros <- soma_parametros + fit
    soma_var <- soma_var + (fit - c(alphas[k], betas[k]))^2
  }
  
  vies_alpha <- round(soma_parametros[1]/N - alphas[k], 4)
  eqm_alpha <-  round(soma_var[1]/N + vies_alpha^2, 4)
  vies_beta <-  round(soma_parametros[2]/N - betas[k], 4)
  eqm_beta <-  round(soma_var[2]/N + vies_beta^2, 4)
  
  df1 <- rbind(df1, c(alphas[k], betas[k], n[j],
                      vies_alpha, eqm_alpha, vies_beta, eqm_beta))
}
}

names(df1)[1] <- "Alpha"; names(df1)[2] <- "Beta"; names(df1)[3] <- "n"
names(df1)[4] <- "Vies Alpha"; names(df1)[5] <- "EQM Alpha"
names(df1)[6] <- "Vies Beta"; names(df1)[7] <- "EQM Beta"

knitr::kable(df1, caption = "Tabela do Algoritmo 1")


#Loop Monte-Carlo Algoritmo 3

df3 <- data.frame()

for (k in 1:3) {

for (j in 1:5){
  
  soma_parametros <- c(0, 0)
  soma_var <- c(0,0)
  
  for(i in 1:N){
    
    x <- Algoritmo3(n[j], alphas[k], betas[k])
    fit <- suppressWarnings(optim(c(alphas[k], betas[k]), loglik, x = x,
                                  method = "Nelder-Mead")$par)
    
    
    soma_parametros <- soma_parametros + fit
    soma_var <- soma_var + (fit - c(alphas[k], betas[k]))^2
  }
  
  vies_alpha <- round(soma_parametros[1]/N - alphas[k], 4)
  eqm_alpha <-  round(soma_var[1]/N + vies_alpha^2, 4)
  vies_beta <-  round(soma_parametros[2]/N - betas[k], 4)
  eqm_beta <-  round(soma_var[2]/N + vies_beta^2, 4)
  
  df3 <- rbind(df3, c(alphas[k], betas[k], n[j],
                      vies_alpha, eqm_alpha, vies_beta, eqm_beta))
}
}

names(df3)[1] <- "Alpha"; names(df3)[2] <- "Beta"; names(df3)[3] <- "n"
names(df3)[4] <- "Vies Alpha"; names(df3)[5] <- "EQM Alpha"
names(df3)[6] <- "Vies Beta"; names(df3)[7] <- "EQM Beta"

knitr::kable(df3, caption = "Tabela do Algoritmo 3")
```


\newpage
# Referências Bibliográficas